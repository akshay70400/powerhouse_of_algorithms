{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6ba160c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.2.1-cp38-cp38-win_amd64.whl (12.2 MB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.2-cp38-cp38-win_amd64.whl (452 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.9.0-py3-none-any.whl (25 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.6-cp38-cp38-win_amd64.whl (21 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (4.59.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp38-cp38-win_amd64.whl (113 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.13-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.5-cp38-cp38-win_amd64.whl (6.6 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (52.0.0.post20210125)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp38-cp38-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "Successfully installed blis-0.7.5 catalogue-2.0.6 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.6 pathy-0.6.1 preshed-3.0.6 smart-open-5.2.1 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0 wasabi-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "331fbd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 04:50:53.456912: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-02-03 04:50:53.480235: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.59.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (20.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.2.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87126091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl (45.7 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from en-core-web-md==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (20.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.59.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.19.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2020.12.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 04:56:30.961577: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-02-03 04:56:30.963242: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.2.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19b5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204fd236",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd24c754",
   "metadata": {},
   "source": [
    "# Containers¶\n",
    "Containers are spaCy objects that contain a large quantity of data about a text. When we analyze texts with the spaCy framework, we create different container objects to do that. Here is a full list of all spaCy containers. We will be focusing on three (emboldened): Doc, Span, and Token.\n",
    "\n",
    "Doc\n",
    "\n",
    "DocBin\n",
    "\n",
    "Example\n",
    "\n",
    "Language\n",
    "\n",
    "Lexeme\n",
    "\n",
    "Span\n",
    "\n",
    "SpanGroup\n",
    "\n",
    "Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb035efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60421abe",
   "metadata": {},
   "source": [
    "# RULES BASED APPROACH\n",
    "Vs Machine learning approach\n",
    "\n",
    "Rules based approach:Ex. RegEx\n",
    "\n",
    "Machine learning based approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8919cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c22331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_md')\n",
    "\n",
    "text='West Chestertenfieldville was referenced in Mr. Deeds'\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92c28542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West Chestertenfieldville GPE\n",
      "Deeds PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee2d658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe('entity_ruler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48efc310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'entity_ruler': []},\n",
       " 'attrs': {'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner', 'entity_ruler'], 'requires': []}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaef3a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    {'label':'GPE','pattern':'West Chestertenfieldville'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df167d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac63ba8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West Chestertenfieldville GPE\n",
      "Deeds PERSON\n"
     ]
    }
   ],
   "source": [
    "doc2=nlp(text)\n",
    "\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d87f1bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West Chestertenfieldville GPE\n",
      "Deeds PERSON\n"
     ]
    }
   ],
   "source": [
    "nlp2 = spacy.load('en_core_web_sm')\n",
    "\n",
    "ruler = nlp2.add_pipe('entity_ruler', before='ner')\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp2(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37364ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West Chestertenfieldville GPE\n",
      "Mr. Deeds FILM\n"
     ]
    }
   ],
   "source": [
    "nlp3 = spacy.load('en_core_web_sm')\n",
    "ruler=nlp3.add_pipe('entity_ruler', before='ner')\n",
    "\n",
    "patterns=[\n",
    "    {'label':'GPE', 'pattern':'West Chestertenfieldville'},\n",
    "    {'label':'FILM','pattern':'Mr. Deeds'}\n",
    "]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc=nlp3(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TOPONYM RESOLUTION\n",
    "toponym : tokens/words that can have multiple labels depending upon the context.\n",
    "Ex. Mr.Deeds can be PERSON or a movie name.\n",
    "This is still unresolved issue in machine learning based.\n",
    "But throught entity ruler we can handle this issue a bit mannually\n",
    "In machine learning we can provide training examples where Mr.Deeds will be movie as well as person name.\n",
    "So that our model can learn from word embeddings of the same.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb08d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94222157",
   "metadata": {},
   "source": [
    "## Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "637a02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ee036b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RegEx cna be used to do similar things but in different way.\n",
    "When linguistic components or the lemma of the word or whether the word is specific type of an entity in parts of speech in such cases we can use matcher over regex.\n",
    "Whereas when you want to extract complicated pattern and that pattern is not dependent upon specific parts of speech in that case you use regex.\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08ebaf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16571425990740197027, 6, 7)]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{'LIKE_EMAIL': True}]\n",
    "matcher.add('EMAIL_ADDRESS', [pattern])\n",
    "\n",
    "doc=nlp('This is my email adress: asnddfkf@aol.com')\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "print(matches)\n",
    "\n",
    "\"\"\"\n",
    "16571425990740197027 (LEXEM), 6 (Start index), 7\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "128edd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMAIL_ADDRESS\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab[matches[0][0]].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42ac7c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India, country that occupies the greater part of South Asia. Its capital is New Delhi, built in the 20th century just south of the historic hub of Old Delhi to serve as India’s administrative centre. Its government is a constitutional republic that represents a highly diverse population consisting of thousands of ethnic groups and likely hundreds of languages. With roughly one-sixth of the world’s total population, India is the second most populous country, after China.\n",
      "It is known from archaeological evidence that a highly sophisticated urbanized culture—the Indus civilization—dominated the northwestern part of the subcontinent from about 2600 to 2000 BCE. From that period on, India functioned as a virtually self-contained political and cultural arena, which gave rise to a distinctive tradition that was associated primarily with Hinduism, the roots of which can largely be traced to the Indus civilization. Other religions, notably Buddhism and Jainism, originated in India—though their presence there is now quite small—and throughout the centuries residents of the subcontinent developed a rich intellectual life in such fields as mathematics, astronomy, architecture, literature, music, and the fine arts.\n",
      "Throughout its history, India was intermittently disturbed by incursions from beyond its northern mountain wall. Especially important was the coming of Islam, brought from the northwest by Arab, Turkish, Persian, and other raiders beginning early in the 8th century CE. Eventually, some of those raiders stayed; by the 13th century much of the subcontinent was under Muslim rule, and the number of Muslims steadily increased. Only after the arrival of the Portuguese navigator Vasco da Gama in 1498 and the subsequent establishment of European maritime supremacy in the region did India become exposed to major external influences arriving by sea, a process that culminated in the decline of the ruling Muslim elite and absorption of the subcontinent within the British Empire.\n",
      "Direct administration by the British, which began in 1858, effected a political and economic unification of the subcontinent. When British rule came to an end in 1947, the subcontinent was partitioned along religious lines into two separate countries—India, with a majority of Hindus, and Pakistan, with a majority of Muslims; the eastern portion of Pakistan later split off to form Bangladesh. Many British institutions stayed in place (such as the parliamentary system of government); English continued to be a widely used lingua franca; and India remained within the Commonwealth. Hindi became the official language (and a number of other local languages achieved official status), while a vibrant English-language intelligentsia thrived.\n",
      "India remains one of the most ethnically diverse countries in the world. Apart from its many religions and sects, India is home to innumerable castes and tribes, as well as to more than a dozen major and hundreds of minor linguistic groups from several language families unrelated to one another. Religious minorities, including Muslims, Christians, Sikhs, Buddhists, and Jains, still account for a significant proportion of the population; collectively, their numbers exceed the populations of all countries except China. Earnest attempts have been made to instill a spirit of nationhood in so varied a population, but tensions between neighbouring groups have remained and at times have resulted in outbreaks of violence. Yet social legislation has done much to alleviate the disabilities previously suffered by formerly “untouchable” castes, tribal populations, women, and other traditionally disadvantaged segments of society. At independence, India was blessed with several leaders of world stature, most notably Mohandas (Mahatma) Gandhi and Jawaharlal Nehru, who were able to galvanize the masses at home and bring prestige to India abroad. The country has played an increasing role in global affairs.\n",
      "Contemporary India’s increasing physical prosperity and cultural dynamism—despite continued domestic challenges and economic inequality—are seen in its well-developed infrastructure and a highly diversified industrial base, in its pool of scientific and engineering personnel (one of the largest in the world), in the pace of its agricultural expansion, and in its rich and vibrant cultural exports of music, literature, and cinema. Though the country’s population remains largely rural, India has three of the most populous and cosmopolitan cities in the world—Mumbai (Bombay), Kolkata (Calcutta), and Delhi. Three other Indian cities—Bengaluru (Bangalore), Chennai (Madras), and Hyderabad—are among the world’s fastest-growing high-technology centres, and most of the world’s major information technology and software companies now have offices in India.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"C:\\\\Users\\\\admin\\\\Desktop\\\\india.txt\", \"r\",encoding='utf8') as f:\n",
    "    text=f.read()\n",
    "    \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc324312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "(451313080118390996, 0, 1) India\n",
      "(451313080118390996, 9, 10) South\n",
      "(451313080118390996, 10, 11) Asia\n",
      "(451313080118390996, 15, 16) New\n",
      "(451313080118390996, 16, 17) Delhi\n",
      "(451313080118390996, 30, 31) Old\n",
      "(451313080118390996, 31, 32) Delhi\n",
      "(451313080118390996, 35, 36) India\n",
      "(451313080118390996, 76, 77) India\n",
      "(451313080118390996, 85, 86) China\n"
     ]
    }
   ],
   "source": [
    "#Task is extract all proper nounds from India.text\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher =Matcher(nlp.vocab)\n",
    "pattern = [{'POS':'PROPN'}]\n",
    "\n",
    "matcher.add('PROPER_NOUN', [pattern])\n",
    "doc=nlp(text)\n",
    "matches=matcher(doc)\n",
    "\n",
    "print(len(matches))\n",
    "\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "However here single word is being captured and not the multi word proper noun\n",
    "For doing that ww will need to add OP\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb4d2cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "(451313080118390996, 0, 1) India\n",
      "(451313080118390996, 9, 10) South\n",
      "(451313080118390996, 9, 11) South Asia\n",
      "(451313080118390996, 10, 11) Asia\n",
      "(451313080118390996, 15, 16) New\n",
      "(451313080118390996, 15, 17) New Delhi\n",
      "(451313080118390996, 16, 17) Delhi\n",
      "(451313080118390996, 30, 31) Old\n",
      "(451313080118390996, 30, 32) Old Delhi\n",
      "(451313080118390996, 31, 32) Delhi\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher =Matcher(nlp.vocab)\n",
    "pattern = [{'POS':'PROPN', 'OP':'+'}]\n",
    "#Here OP operator determines how often to match a token pattern\n",
    "# + reuires pattern to be match 1 or more times\n",
    "\n",
    "matcher.add('PROPER_NOUN', [pattern])\n",
    "doc=nlp(text)\n",
    "matches=matcher(doc)\n",
    "\n",
    "print(len(matches))\n",
    "\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])\n",
    "    \n",
    "\"\"\"\n",
    "However even this is matching all possible combinations\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a96fd333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "(451313080118390996, 302, 305) Vasco da Gama\n",
      "(451313080118390996, 9, 11) South Asia\n",
      "(451313080118390996, 15, 17) New Delhi\n",
      "(451313080118390996, 30, 32) Old Delhi\n",
      "(451313080118390996, 349, 351) British Empire\n",
      "(451313080118390996, 665, 667) Jawaharlal Nehru\n",
      "(451313080118390996, 696, 698) Contemporary India\n",
      "(451313080118390996, 0, 1) India\n",
      "(451313080118390996, 35, 36) India\n",
      "(451313080118390996, 76, 77) India\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher =Matcher(nlp.vocab)\n",
    "pattern = [{'POS':'PROPN', 'OP':'+'}]\n",
    "\n",
    "matcher.add('PROPER_NOUN', [pattern], greedy=\"LONGEST\")\n",
    "#greedy=Longest here matches the only pattern which has longest length\n",
    "doc=nlp(text)\n",
    "matches=matcher(doc)\n",
    "\n",
    "print(len(matches))\n",
    "\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a834069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "(451313080118390996, 0, 1) India\n",
      "(451313080118390996, 9, 11) South Asia\n",
      "(451313080118390996, 15, 17) New Delhi\n",
      "(451313080118390996, 30, 32) Old Delhi\n",
      "(451313080118390996, 35, 36) India\n",
      "(451313080118390996, 76, 77) India\n",
      "(451313080118390996, 85, 86) China\n",
      "(451313080118390996, 102, 103) Indus\n",
      "(451313080118390996, 117, 118) BCE\n",
      "(451313080118390996, 124, 125) India\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher =Matcher(nlp.vocab)\n",
    "pattern = [{'POS':'PROPN', 'OP':'+'}]\n",
    "\n",
    "matcher.add('PROPER_NOUN', [pattern], greedy=\"LONGEST\")\n",
    "#greedy=Longest here matches the only pattern which has longest length\n",
    "doc=nlp(text)\n",
    "matches=matcher(doc)\n",
    "matches.sort(key=lambda x: x[1]) #Sorts based on start index\n",
    "\n",
    "print(len(matches))\n",
    "\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54934ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "(451313080118390996, 4, 5) occupies\n",
      "(451313080118390996, 18, 19) built\n",
      "(451313080118390996, 33, 34) serve\n",
      "(451313080118390996, 47, 48) represents\n",
      "(451313080118390996, 52, 53) consisting\n",
      "(451313080118390996, 90, 91) known\n",
      "(451313080118390996, 105, 106) dominated\n",
      "(451313080118390996, 125, 126) functioned\n",
      "(451313080118390996, 131, 132) contained\n",
      "(451313080118390996, 138, 139) gave\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher =Matcher(nlp.vocab)\n",
    "pattern = [{'POS':'PROPN', 'OP':'+', 'POS':'VERB'}]\n",
    "#This helps find where in the text proper noun is being proceeded by a verb\n",
    "\n",
    "matcher.add('PROPER_NOUN', [pattern], greedy=\"LONGEST\")\n",
    "doc=nlp(text)\n",
    "matches=matcher(doc)\n",
    "matches.sort(key=lambda x: x[1])\n",
    "print(len(matches))\n",
    "\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68411b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "#TASK: in given json, find out quotations and also who are saying those quotations\n",
    "\n",
    "# import json\n",
    "# with open(\"alice.json\", \"r\") as f:\n",
    "#     data=json.load(f)\n",
    "    \n",
    "#text=data[0][2][0]\n",
    "text=\"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7e26723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "text = text.replace(\"`\", \"'\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4839dd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(451313080118390996, 47, 58) 'and what is the use of a book,'\n",
      "(451313080118390996, 60, 67) 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher =Matcher(nlp.vocab)\n",
    "pattern = [{\"ORTH\":\"'\"},\n",
    "          {\"IS_ALPHA\":True, \"OP\":\"+\"},\n",
    "          {\"IS_PUNCT\":True, \"OP\":\"*\"},\n",
    "          {\"ORTH\":\"'\"}]\n",
    "\n",
    "matcher.add('PROPER_NOUN', [pattern], greedy=\"LONGEST\")\n",
    "doc=nlp(text)\n",
    "matches=matcher(doc)\n",
    "matches.sort(key=lambda x: x[1])\n",
    "print(len(matches))\n",
    "\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "96dda8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(451313080118390996, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "speak_lemmas = [\"think\", \"say\"]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher =Matcher(nlp.vocab)\n",
    "pattern = [{\"ORTH\":\"'\"},\n",
    "          {\"IS_ALPHA\":True, \"OP\":\"+\"},\n",
    "          {\"IS_PUNCT\":True, \"OP\":\"*\"},\n",
    "          {\"ORTH\":\"'\"},\n",
    "          {\"POS\":\"VERB\", \"LEMMA\": {\"IN\":speak_lemmas}},\n",
    "          {\"POS\":\"PROPN\", \"OP\":\"+\"},\n",
    "           \n",
    "          {\"ORTH\":\"'\"},\n",
    "          {\"IS_ALPHA\":True, \"OP\":\"+\"},\n",
    "          {\"IS_PUNCT\":True, \"OP\":\"*\"},\n",
    "          {\"ORTH\":\"'\"},\n",
    "          ]\n",
    "\n",
    "matcher.add('PROPER_NOUN', [pattern], greedy=\"LONGEST\")\n",
    "doc=nlp(text)\n",
    "matches=matcher(doc)\n",
    "matches.sort(key=lambda x: x[1])\n",
    "print(len(matches))\n",
    "\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e90b2a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-eeaef0f5e1d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdoc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmatches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "for text in data[0][2]:\n",
    "    text=text.replace(\"`\",\"'\")\n",
    "    doc=nlp(text)\n",
    "    matches=matcher(doc)\n",
    "    print(len(matches))\n",
    "    matches.sort(key=lambda x: x[1])\n",
    "    for match in matches[:10]:\n",
    "        print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe7e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58ddf22f",
   "metadata": {},
   "source": [
    "# CUSTOM COMPONENTS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "643e8ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4f671ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc=nlp('Britain is a place. Marry Jane is a doctor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e0fda68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Britain GPE\n",
      "Marry Jane PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7bb444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task given is to flag all GPE as LOC because all other entitties in project has label as location\n",
    "#So we will need custom components here\n",
    "\n",
    "from spacy.language import Language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccf748f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"remove_gpe\")\n",
    "def remove_gpe(doc):\n",
    "    original_ents = list(doc.ents)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\":\n",
    "            original_ents.remove(ent)\n",
    "    doc.ents = original_ents\n",
    "    return(doc)\n",
    "\n",
    "#This is how we change the doc objects along the way in th pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12c567ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.remove_gpe(doc)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"remove_gpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a971edd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'remove_gpe': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'remove_gpe': []},\n",
       " 'attrs': {'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28a6a14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marry Jane PERSON\n"
     ]
    }
   ],
   "source": [
    "doc=nlp('Britain is a place. Marry Jane is a doctor.')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30d4843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('new_eng_core_web_sm')\n",
    "\n",
    "#This way you are saving the new pipeline with custom components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03490a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61ad6f63",
   "metadata": {},
   "source": [
    "# REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121cc99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We prefer using regex when the pattern matching that you want to do is independent of the lemma, pos or any other linguistic feature the spacy is going to use.\n",
    "#But you are after any linguistic feature matching then use the spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4324a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22890a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "647b88e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 11), match='Paul Newman'>\n",
      "<re.Match object; span=(39, 53), match='Paul Hollywood'>\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"Paul [A-Z]\\w+\"\n",
    "\n",
    "matches =re.finditer(pattern, text)\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "178ca35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "646a4c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "Paul Newman\n",
      "Paul Hollywood\n",
      "(Paul Newman, Paul Hollywood)\n",
      "Paul Newman PERSON\n",
      "Paul Hollywood PERSON\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "doc=nlp(text)\n",
    "print(doc.ents)\n",
    "original_ents=list(doc.ents)\n",
    "\n",
    "mwt_ents=[]\n",
    "\n",
    "for match in re.finditer(pattern, doc.text):\n",
    "    start, end=match.span()\n",
    "    span=doc.char_span(start, end)\n",
    "    print(span)\n",
    "    if span is not None:\n",
    "        mwt_ents.append((span.start, span.end, span.text))\n",
    "        \n",
    "for ent in mwt_ents:\n",
    "    start, end, name = ent\n",
    "    per_ent = Span(doc, start, end, label='PERSON')\n",
    "    original_ents.append(per_ent)\n",
    "doc.ents = original_ents\n",
    "print(doc.ents)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f917b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2, 'Paul Newman'), (8, 10, 'Paul Hollywood')]\n"
     ]
    }
   ],
   "source": [
    "print(mwt_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ed43852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component('paul_ner')\n",
    "def paul_ner(doc):\n",
    "    original_ents=list(doc.ents)\n",
    "    pattern = r\"Paul [A-Z]\\w+\"\n",
    "\n",
    "    mwt_ents=[]\n",
    "\n",
    "    for match in re.finditer(pattern, doc.text):\n",
    "        start, end=match.span()\n",
    "        span=doc.char_span(start, end)\n",
    "        print(span)\n",
    "        if span is not None:\n",
    "            mwt_ents.append((span.start, span.end, span.text))\n",
    "\n",
    "    for ent in mwt_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label='PERSON')\n",
    "        original_ents.append(per_ent)\n",
    "    doc.ents = original_ents\n",
    "    return (doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eacda734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.paul_ner(doc)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2 = spacy.blank(\"en\")\n",
    "nlp2.add_pipe(\"paul_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a4ec8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Newman\n",
      "Paul Hollywood\n",
      "(Paul Newman, Paul Hollywood)\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp2(text)\n",
    "print(doc2.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c7e046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5db13a49",
   "metadata": {},
   "source": [
    "# FINANCIAL ANALYSIS NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91bce71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('stocks.tsv')\n",
    "#tsv means tab separated values file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a00df",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = df.Symbol.tolist()\n",
    "companies = df.company.tolist()\n",
    "print(symbols[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535981b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en')\n",
    "ruler=nlp.add_pipe('entity_ruler')\n",
    "pattern = \n",
    "\n",
    "\n",
    "for symbol in symbols:\n",
    "    patterns.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39297896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64221a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af140b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f787e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd5adad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31285bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504639d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
